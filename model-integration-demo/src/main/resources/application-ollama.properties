# Ollama 本地模型配置
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=llama2
spring.ai.ollama.chat.options.temperature=0.7

# 环境信息
info.model.provider=Ollama
info.model.name=llama2
info.model.type=Local Chat Model
info.model.deployment=Self-hosted